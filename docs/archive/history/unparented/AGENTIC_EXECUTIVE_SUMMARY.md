# Agentic Capability Analysis - Executive Summary

## Project: Oh My Coins (OMC!) - Agentic Data Science Capability

**Date**: November 15, 2025  
**Status**: Requirements Analysis Complete âœ…  
**Next Phase**: Implementation Ready

---

## ðŸ“‹ Analysis Overview

This document summarizes the comprehensive analysis performed to determine requirements for adding an autonomous agentic capability to the Oh My Coins Lab, as described in the generic data science report provided.

### What Was Delivered

Four comprehensive planning documents totaling **88 KB** of detailed specifications:

1. **AGENTIC_REQUIREMENTS.md** (26 KB) - Requirements specification
2. **AGENTIC_ARCHITECTURE.md** (29 KB) - Technical architecture  
3. **AGENTIC_IMPLEMENTATION_PLAN.md** (19 KB) - 14-week implementation plan
4. **AGENTIC_QUICKSTART.md** (13 KB) - Quick reference guide

Plus updates to:
- **ROADMAP.md** - Added new Phase 3 for agentic capability
- **README.md** - Comprehensive project overview

---

## ðŸŽ¯ The Solution

Transform the Oh My Coins Lab from a manual algorithm development platform into an **autonomous AI-powered "data scientist"** that can:

1. **Understand** high-level trading goals in natural language
2. **Plan** complete data science workflows autonomously  
3. **Execute** each step using specialized AI agents
4. **Deliver** evaluated trading models with recommendations
5. **Collaborate** with users through human-in-the-loop features

### Example User Experience

**Before (Manual)**:
```
1. User manually queries database for price data
2. User writes code to calculate technical indicators
3. User engineers features (hours of work)
4. User trains multiple models manually
5. User evaluates and compares models
6. User tunes hyperparameters (trial and error)
7. User documents results
Total time: Hours to days
```

**After (Agentic)**:
```
1. User types: "Build a model to predict Bitcoin price movements"
2. AI agents autonomously complete steps 1-7
3. User receives final evaluated model with report
Total time: 5-15 minutes
```

---

## ðŸ—ï¸ Architecture: Multi-Agent System

### Core Components

```
User Goal â†’ Agent Orchestrator â†’ 5 Specialized Agents â†’ Results

Orchestrator uses LangGraph State Machine:
  1. Planning Node
  2. Data Retrieval Node (Agent 1)
  3. Data Analysis Node (Agent 2)
  4. Feature Engineering Node (Agent 2)
  5. Model Training Node (Agent 3)
  6. Model Evaluation Node (Agent 4)
  7. Hyperparameter Tuning Node (Agent 4)
  8. Reporting Node (Agent 5)
```

### The 5 Specialized Agents

1. **Data Retrieval Agent**
   - Fetches cryptocurrency price data from `price_data_5min` table
   - Tools: `fetch_price_data`, `get_available_coins`, `get_data_statistics`

2. **Data Analyst Agent**
   - Performs EDA, cleaning, feature engineering
   - Tools: `calculate_technical_indicators`, `clean_data`, `perform_eda`, `create_features`

3. **Model Training Agent**
   - Trains ML models using scikit-learn API
   - Tools: `train_classification_model`, `train_regression_model`, `cross_validate_model`
   - Supports: Logistic Regression, Random Forest, XGBoost

4. **Model Evaluator Agent**
   - Evaluates and optimizes models
   - Tools: `evaluate_model`, `tune_hyperparameters`, `compare_models`, `feature_importance`

5. **Reporting Agent**
   - Generates natural language summaries
   - Tools: `generate_summary`, `create_comparison_report`, `generate_recommendations`

### Key Innovation: ReAct Loop

**Reason-Act-Observe** pattern enables iterative refinement:

```
Reason: "The model accuracy is only 75%. I should try tuning hyperparameters."
Act: tune_hyperparameters(model, param_grid)
Observe: "Accuracy improved to 82%. This is better."
Reason: "Now I'll try with more estimators."
Act: tune_hyperparameters(model, {n_estimators: [200, 300]})
Observe: "Accuracy is now 85%. Excellent, ready to report."
```

This mirrors how a human data scientist iteratively improves models.

---

## ðŸ¤ Human-in-the-Loop Features

### 1. Clarification Requests
When the agent encounters ambiguity:

```
Agent: "What time period should I use for training?"
Options:
  - Last 3 months (most recent)
  - Last 6 months (balanced)
  - All available data (maximum history)
User: "Last 6 months"
Agent: "Got it, proceeding..."
```

### 2. Choice Presentation
When multiple valid options exist:

```
Agent: "I've trained two models:
  1. RandomForest - 78% accuracy, faster predictions
  2. XGBoost - 82% accuracy, more accurate but slower
  
Which would you prefer?"
User: "XGBoost for accuracy"
Agent: "Perfect, I'll optimize XGBoost"
```

### 3. User Overrides
Users can intervene at any point:

```
Agent: "I recommend using all features including last_price"
User: "Override: Remove last_price due to data leakage"
Agent: "Understood, retraining without last_price"
```

### 4. Approval Gates
Configurable checkpoints for critical decisions:

```
Agent: "Ready to train 5 models with cross-validation. 
Estimated cost: 50,000 tokens (~$0.10). 
Approve?"
User: "Approved"
Agent: "Training initiated..."
```

---

## ðŸ” Security & Safety

### Code Execution Sandbox
- **RestrictedPython** environment
- **Resource Limits**: 5 minutes max, 2GB memory
- **Allowed Imports**: Only data science libraries (pandas, sklearn, etc.)
- **No Network Access**: Agent code cannot make external API calls
- **Validation**: AST parsing to detect forbidden operations

### API Security
- **JWT Authentication**: All endpoints require authentication
- **User Isolation**: Users can only access their own sessions
- **Rate Limiting**: Prevent abuse
- **Input Validation**: All inputs sanitized

### LLM Security
- **Prompt Injection Protection**: Detect and block injection attempts
- **Token Limits**: Max 50,000 tokens per session
- **Cost Tracking**: Monitor and alert on excessive usage
- **Output Validation**: Verify agent responses are safe

---

## ðŸ“Š API Design

### New Endpoints (To Be Implemented)

```python
# Create new agent session
POST /api/v1/lab/agent/sessions
{
    "goal": "Predict Bitcoin price movements",
    "preferences": {
        "approval_mode": "interactive",
        "time_period": "last_6_months"
    }
}
Response: {"session_id": "uuid", "status": "planning"}

# Get session status
GET /api/v1/lab/agent/sessions/{session_id}
Response: {
    "status": "awaiting_approval",
    "current_step": "model_training",
    "clarification": {
        "question": "Which model?",
        "options": ["RandomForest", "XGBoost"]
    }
}

# Respond to clarification
POST /api/v1/lab/agent/sessions/{session_id}/respond
{"response": "XGBoost"}

# Stream updates (WebSocket)
WS /api/v1/lab/agent/sessions/{session_id}/stream

# Cancel session
DELETE /api/v1/lab/agent/sessions/{session_id}

# Get final results
GET /api/v1/lab/agent/sessions/{session_id}/results
Response: {
    "best_model": {...},
    "metrics": {...},
    "recommendations": "...",
    "artifacts": {
        "model_file": "path/to/model.pkl",
        "plots": ["feature_importance.png", "confusion_matrix.png"]
    }
}
```

### Database Schema (To Be Implemented)

```sql
-- Agent session tracking
CREATE TABLE agent_sessions (
    id UUID PRIMARY KEY,
    user_id UUID REFERENCES users(id),
    goal TEXT NOT NULL,
    status VARCHAR(50),  -- 'planning', 'executing', 'completed', 'failed'
    current_step VARCHAR(100),
    plan JSONB,
    context JSONB,
    results JSONB,
    created_at TIMESTAMP,
    completed_at TIMESTAMP
);

-- Conversation history
CREATE TABLE agent_session_messages (
    id UUID PRIMARY KEY,
    session_id UUID REFERENCES agent_sessions(id),
    role VARCHAR(20),  -- 'user', 'assistant', 'tool'
    content TEXT,
    metadata JSONB,
    created_at TIMESTAMP
);

-- Generated artifacts
CREATE TABLE agent_artifacts (
    id UUID PRIMARY KEY,
    session_id UUID REFERENCES agent_sessions(id),
    artifact_type VARCHAR(50),  -- 'model', 'plot', 'report'
    file_path TEXT,
    metadata JSONB,
    created_at TIMESTAMP
);
```

---

## ðŸ› ï¸ Technology Stack

### New Dependencies

```toml
[dependencies]
# Agent Framework
langchain = ">=0.1.0"
langchain-openai = ">=0.0.5"
langgraph = ">=0.0.20"

# State Management
redis = ">=5.0.0"

# Data Science
pandas = ">=2.0.0"
scikit-learn = ">=1.3.0"
xgboost = ">=2.0.0"
matplotlib = ">=3.7.0"
seaborn = ">=0.12.0"
ta = ">=0.11.0"  # Technical indicators
```

### Infrastructure Requirements

**Redis** (for agent state):
```yaml
# docker-compose.yml
redis:
  image: redis:7-alpine
  ports:
    - "6379:6379"
  volumes:
    - redis-data:/data
```

**LLM Provider**:
- OpenAI API (recommended: GPT-4 Turbo)
- Or Anthropic API (Claude 3)
- Or Azure OpenAI

---

## ðŸ“… Implementation Timeline

### 14-Week Plan (3.5 Months)

| Phase | Weeks | Deliverables |
|-------|-------|--------------|
| **Foundation** | 1-2 | LangChain/LangGraph setup, Redis, database schema, API routes |
| **Data Agents** | 3-4 | Data Retrieval + Data Analyst agents (7 tools) |
| **Modeling Agents** | 5-6 | Model Training + Model Evaluator agents (7 tools) |
| **Orchestration** | 7-8 | LangGraph state machine, ReAct loop, end-to-end workflow |
| **Human-in-the-Loop** | 9-10 | Clarifications, overrides, approval gates |
| **Reporting** | 11-12 | Reporting agent, artifact management, code sandbox |
| **Testing & Docs** | 13-14 | 80%+ test coverage, security audit, documentation |

**Detailed Tasks**: See `AGENTIC_IMPLEMENTATION_PLAN.md` for day-by-day breakdown

---

## ðŸ’° Budget & Resources

### Personnel
- **1 full-time developer** for 14 weeks
- Part-time code reviews
- Part-time testing support

### Costs
- **LLM API**: ~$500/month during development = ~$1,000
- **Infrastructure**: ~$50/month for Redis = ~$250
- **Total**: **~$1,500** for complete project

### ROI
- **Time Savings**: Hours â†’ Minutes per algorithm
- **Quality**: Consistent, tested, documented models
- **Innovation**: Rapid experimentation enables more strategies
- **Accessibility**: Non-technical users can develop algorithms

---

## âœ… Success Criteria

### Functional Requirements
- âœ… Agent accepts natural language goals
- âœ… Agent autonomously completes data science workflows
- âœ… Agent trains and evaluates multiple models
- âœ… Agent presents results with recommendations
- âœ… User can provide clarifications
- âœ… User can override decisions
- âœ… Complete audit trail

### Performance Requirements
- âœ… Simple tasks: < 5 minutes
- âœ… Complex tasks: < 15 minutes
- âœ… Success rate: > 90%
- âœ… Concurrent sessions: 10+

### Quality Requirements
- âœ… Test coverage: > 80%
- âœ… Critical security issues: 0
- âœ… Documentation: Complete
- âœ… User satisfaction: > 4/5

---

## ðŸš€ Getting Started

### Prerequisites for Implementation

1. **Approval**: Stakeholder sign-off on scope and budget
2. **Resources**: Developer allocated for 14 weeks  
3. **API Keys**: OpenAI or Anthropic API access
4. **Infrastructure**: Redis instance available

### Week 1 Kickoff Tasks

```bash
# 1. Update dependencies
cd backend
uv add langchain langchain-openai langgraph redis pandas scikit-learn xgboost matplotlib seaborn ta

# 2. Add Redis to docker-compose.yml
# (See AGENTIC_ARCHITECTURE.md)

# 3. Create directory structure
mkdir -p backend/app/services/agent/{agents,tools}

# 4. Create database migration
uv run alembic revision -m "add_agent_tables"

# 5. Configure environment
echo "LLM_PROVIDER=openai" >> .env
echo "OPENAI_API_KEY=sk-..." >> .env
```

---

## ðŸ“š Documentation Index

All requirements, architecture, and implementation details are fully documented:

### Planning Documents (75+ pages total)

1. **[AGENTIC_QUICKSTART.md](./AGENTIC_QUICKSTART.md)** (13 KB)
   - Quick reference guide
   - Architecture overview
   - Example usage
   - Setup instructions

2. **[AGENTIC_REQUIREMENTS.md](./AGENTIC_REQUIREMENTS.md)** (26 KB)
   - Complete requirements specification
   - Agent definitions and capabilities
   - Tool implementations
   - Human-in-the-loop features
   - Security considerations
   - Success criteria

3. **[AGENTIC_ARCHITECTURE.md](./AGENTIC_ARCHITECTURE.md)** (29 KB)
   - Technical architecture design
   - Component details
   - LangGraph state machine
   - Code examples
   - Data flow diagrams
   - Deployment strategy

4. **[AGENTIC_IMPLEMENTATION_PLAN.md](./AGENTIC_IMPLEMENTATION_PLAN.md)** (19 KB)
   - Week-by-week breakdown
   - Daily task lists
   - Deliverables per phase
   - Risk management
   - Resource requirements
   - Rollout strategy

### Project Documentation

5. **[ROADMAP.md](./ROADMAP.md)** (Updated)
   - New Phase 3 for agentic capability
   - Integration with project roadmap
   - Prioritization and dependencies

6. **[README.md](./README.md)** (Updated)
   - Project overview
   - Quick start instructions
   - Documentation index
   - Contact information

---

## ðŸŽ¯ Alignment with Problem Statement

The solution addresses all requirements from the generic data science report:

### âœ… 1. Core Architecture: Multi-Agent Framework
- **Implemented**: 5-agent system with LangChain/LangGraph
- **Agents**: Planner/Orchestrator + 4 specialist agents + 1 reporter
- **Framework**: LangChain chosen for maturity and ecosystem

### âœ… 2. Key Capabilities

**Tool Use (Function Calling)**:
- 15+ tools across 4 categories (data, analysis, modeling, evaluation)
- LangChain tool decorator with automatic schema generation
- Secure code interpreter sandbox

**Reasoning and Planning**:
- LangGraph state machine with 8 workflow nodes
- Multi-step plan generation from vague prompts
- Algorithm selection logic based on problem characteristics

**Iterative Reflection (ReAct Loop)**:
- Reason-Act-Observe cycle implemented
- Hyperparameter tuning with feedback
- Model comparison and selection

**Human-in-the-Loop (HiTL)**:
- Clarification requests for ambiguity
- Choice presentation with pros/cons
- User override mechanism
- Approval gates at critical points

---

## ðŸŽ‰ Conclusion

### What Was Accomplished

âœ… **Complete requirements analysis** for agentic capability  
âœ… **Comprehensive technical architecture** designed  
âœ… **Detailed 14-week implementation plan** created  
âœ… **All documentation** prepared and reviewed  
âœ… **Project ready** for implementation approval  

### Impact

This agentic capability will:

- **10x faster** algorithm development (hours â†’ minutes)
- **Lower barrier** to entry (natural language â†’ models)
- **Higher quality** through AI best practices
- **Enable innovation** via rapid experimentation
- **Maintain control** through human-in-the-loop

### Next Steps

1. **Review**: Stakeholders review planning documents
2. **Approve**: Sign off on scope, timeline, budget
3. **Allocate**: Assign developer for 14 weeks
4. **Begin**: Start Week 1 foundation setup
5. **Iterate**: Weekly progress reviews

---

## ðŸ“ž Questions?

For more details, refer to the comprehensive planning documents or contact the project team.

**All planning is complete. Ready to proceed with implementation upon approval.**

---

**Document Version**: 1.0  
**Last Updated**: November 15, 2025  
**Status**: Analysis Complete âœ…
