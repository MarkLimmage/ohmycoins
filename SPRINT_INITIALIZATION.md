# Sprint 2.6 Initialization

**Sprint Status:** READY TO START  
**Sprint Dates:** 2026-01-10 to 2026-01-24 (2 weeks)  
**Pre-Sprint Baseline:** 565 passing, 18 failing, 77 errors (merged main branch)

---

## ðŸŽ¯ Sprint 2.6 Objectives

**Primary Goal:** Achieve production readiness through test hardening, 4-ledger completion, and infrastructure finalization.

**Success Criteria:**
- Test suite: <10 failing tests, <20 errors (currently: 18 failing, 77 errors)
- All 4 ledgers operational with quality monitoring implemented
- Agent-data integration functional (agent can query all ledgers)
- Terraform secrets module complete and deployed
- Production deployment runbook validated

**Key Dependencies:**
- Track A â†’ Track B: Schema stability maintained, seed data fixes enable agent testing
- Track B â†’ Track A: Agent tools require ledger data access patterns
- Track C: Secrets management unblocks OpenAI integration for all agent tests

---

## ðŸ“‹ Track Boundaries & Current Status

### Track A: Data & Backend (OMC-Data-Specialist)
**Primary Directories:**
- `backend/app/services/collectors/` (catalyst/, exchange/, glass/, human/)
- `backend/app/models.py` (ledger models only)
- `backend/tests/services/collectors/`
- `backend/tests/utils/test_seed_data.py`

**Current Assets:**
- âœ… SEC API collector: `collectors/catalyst/sec_api.py`
- âœ… CoinSpot announcements: `collectors/catalyst/coinspot_announcements.py`
- âœ… Quality monitor skeleton: `collectors/quality_monitor.py`
- âš ï¸ Seed data tests: 7 failures (idempotency issues)
- âš ï¸ PnL tests: 20 errors (calculation/session issues)

### Track B: Agentic AI (OMC-ML-Scientist)
**Primary Directories:**
- `backend/app/services/agent/` (orchestrator.py, agents/, tools/, langgraph_workflow.py)
- `backend/tests/services/agent/`

**Current Assets:**
- âœ… Orchestrator: run_workflow(), execute_step(), get_session_state()
- âœ… Integration tests: 19/20 passing
- âš ï¸ Redis performance test: 1 failure (async context issue)
- âš ï¸ Security tests: 4 errors (needs investigation)

### Track C: Infrastructure (OMC-DevOps-Engineer)
**Primary Directories:**
- `infrastructure/terraform/modules/secrets/` (currently EMPTY)
- `infrastructure/terraform/environments/`
- `.github/workflows/`

**Current Assets:**
- âœ… ECS base deployment: VPC, RDS, Redis, ECS modules complete
- âœ… Environment template: `.env.template` exists
- âœ… Configuration files: `pytest.ini`, deployment docs
- âš ï¸ Secrets module: Empty directory (main.tf, variables.tf, outputs.tf needed)

---

## ðŸ”§ Developer A: Data Specialist Sprint 2.6

**Role:** OMC-Data-Specialist  
**Focus:** Fix Test Regressions + Complete 4-Ledger Quality Monitoring

---

### ðŸ“š Context Review (Read These First)

1. **Sprint History:** [CURRENT_SPRINT.md](CURRENT_SPRINT.md) lines 80-216 - Review P2 follow-up items from Sprint 2.5
2. **Technical Constraints:** [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md#section-9-technical-constraints-best-practices) - SQLModel relationship patterns, async testing
3. **Test Patterns:** [docs/TESTING.md](docs/TESTING.md) lines 1-50 - Async mock patterns, known issues

---

### ðŸŽ¯ Sprint 2.6 Objectives

#### **PRIORITY 1: Fix Seed Data Test Failures** (P2 from Sprint 2.5)
**File:** [backend/tests/utils/test_seed_data.py](backend/tests/utils/test_seed_data.py)  
**Current Status:** 7 tests failing  
**Issue:** `test_generate_users` reports "Superuser already exists" - idempotency problem

**Tasks:**
1. Investigate seed data generation logic in `backend/app/initial_data.py`
2. Ensure idempotent user creation (check existing before insert)
3. Review relationship queries (lines 167-181) - ensure consistent with SQLModel unidirectional pattern
4. Validate no cascade issues from previous relationship fixes

**Acceptance Criteria:**
- [ ] All 7 seed data tests pass
- [ ] `test_generate_users` handles existing superuser gracefully
- [ ] No new relationship warnings in test output

**Validation Command:**
```bash
cd backend
pytest tests/utils/test_seed_data.py -v --tb=short
```

**Expected Outcome:** 7/7 passing (currently 0/7)

---

#### **PRIORITY 2: Resolve PnL Calculation Errors** (P2 from Sprint 2.5)
**File:** [backend/tests/services/trading/test_pnl.py](backend/tests/services/trading/test_pnl.py)  
**Current Status:** 20 errors  
**Issue:** Database session or calculation errors preventing test execution

**Tasks:**
1. Run PnL tests in isolation to capture full error output
2. Check if errors are cascade effect from seed data issues (run after P1 completion)
3. Review session management in PnL calculation service
4. Verify Position/Order model relationship queries work with unidirectional pattern

**Acceptance Criteria:**
- [ ] 0 errors in test_pnl.py (currently 20)
- [ ] All PnL calculation tests pass or fail gracefully
- [ ] No database session management issues

**Validation Command:**
```bash
cd backend
pytest tests/services/trading/test_pnl.py -v --tb=long
```

**Expected Outcome:** 0 errors, tests either pass or fail with clear assertions

---

#### **PRIORITY 3: Implement Quality Monitor Comprehensive Checks**
**File:** [backend/app/services/collectors/quality_monitor.py](backend/app/services/collectors/quality_monitor.py)  
**Current Status:** Skeleton implementation exists  
**Issue:** Quality checks not comprehensive for 4-ledger system

**Tasks:**
1. Review existing quality_monitor.py implementation
2. Add checks for all 4 ledgers (Glass, Human, Catalyst, Exchange):
   - Data freshness (timestamp within expected window)
   - Completeness (expected fields populated)
   - Consistency (cross-ledger validation where applicable)
3. Implement alert thresholds and logging
4. Create unit tests in `tests/services/collectors/test_quality_monitor.py`

**Acceptance Criteria:**
- [ ] Quality checks implemented for all 4 ledgers
- [ ] Alert thresholds configurable via environment variables
- [ ] 10+ unit tests covering quality check logic
- [ ] Integration test validates quality monitor runs without errors

**Validation Command:**
```bash
cd backend
pytest tests/services/collectors/test_quality_monitor.py -v
```

**Expected Outcome:** 10+ new passing tests for quality monitoring

---

#### **PRIORITY 4: Enhance Catalyst Collectors (If Needed)**
**Files:**
- [backend/app/services/collectors/catalyst/sec_api.py](backend/app/services/collectors/catalyst/sec_api.py)
- [backend/app/services/collectors/catalyst/coinspot_announcements.py](backend/app/services/collectors/catalyst/coinspot_announcements.py)

**Current Status:** Basic implementations exist  
**Assessment Needed:** Review and determine if enhancement is required for Sprint 2.6 scope

**Tasks:**
1. Review SEC API collector error handling and rate limiting
2. Review CoinSpot announcements parsing and storage
3. Add missing test coverage if <80%
4. Document any limitations or known issues

**Acceptance Criteria:**
- [ ] Code review completed with findings documented
- [ ] Test coverage >80% for both collectors
- [ ] Error handling and retry logic validated

**Validation Command:**
```bash
cd backend
pytest tests/services/collectors/catalyst/ -v --cov=app/services/collectors/catalyst
```

**Expected Outcome:** >80% coverage, all tests passing

---

### ðŸ“Š Track A Test Targets

**Baseline (Current):**
- Total: 565 passing, 18 failing, 77 errors
- Your domain: ~120 tests (collectors + utils + trading/pnl)

**Sprint 2.6 Target:**
- Fix 27 issues (7 seed data + 20 PnL errors)
- Add 10+ quality monitor tests
- **End State:** ~137 passing tests in your domain, 0 errors

**Daily Validation:**
```bash
cd backend
# Quick check your test domains
pytest tests/utils/test_seed_data.py tests/services/trading/test_pnl.py tests/services/collectors/ --tb=short -q

# Full baseline comparison
pytest --tb=short --quiet | tee test_output.txt
```

---

### ðŸ”— Integration Points

**Your Outputs â†’ Track B:**
- Schema stability: No breaking changes to `models.py` without coordination
- Seed data: Track B agents need valid test data from your seed functions
- Quality metrics: Track B may query quality_monitor results

**Track B â†’ Your Inputs:**
- Agent tools may query your ledger tables (read-only access)
- Coordinate on any new model fields needed for agent queries

**Track C â†’ Your Dependencies:**
- No blockers: infrastructure work is independent
- Secrets management (Track C P1) will eventually inject DB credentials

---

### ðŸ“ Definition of Done

**Code Quality:**
- [ ] All modified code passes `bash scripts/format.sh`
- [ ] All modified code passes `bash scripts/lint.sh`
- [ ] No new SQLModel relationship warnings

**Testing:**
- [ ] Seed data: 7 tests fixed (0 failures)
- [ ] PnL: 20 errors resolved (0 errors)
- [ ] Quality monitor: 10+ new tests passing
- [ ] All collector tests maintain >80% coverage

**Documentation:**
- [ ] Update [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) if new patterns discovered
- [ ] Document quality monitor thresholds in code comments
- [ ] Update CURRENT_SPRINT.md with final test counts

**Commit Strategy:**
- Commit P1 and P2 fixes separately (seed data, then PnL)
- Commit quality monitor implementation as single feature
- Create PR titled "Track A Sprint 2.6: Test Fixes + Quality Monitor"

---

## ðŸ¤– Developer B: ML Scientist Sprint 2.6

**Role:** OMC-ML-Scientist  
**Focus:** Agent-Data Integration + Fix Remaining Test Issues

---

### ðŸ“š Context Review (Read These First)

1. **Sprint History:** [CURRENT_SPRINT.md](CURRENT_SPRINT.md) lines 80-216 - Review Track B accomplishments and P2 items
2. **Agent Architecture:** [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) - Section on agent system design
3. **Test Status:** [docs/TESTING.md](docs/TESTING.md) - Current 19/20 integration test pass rate

---

### ðŸŽ¯ Sprint 2.6 Objectives

#### **PRIORITY 1: Fix Redis Performance Test** (P2 from Sprint 2.5)
**File:** [backend/tests/services/agent/integration/test_performance.py](backend/tests/services/agent/integration/test_performance.py)  
**Test:** `test_session_state_retrieval_performance`  
**Current Status:** 1 test failing  
**Issue:** Redis connection from async context - wrong number of arguments for `get_session_state()`

**Tasks:**
1. Review orchestrator.py `get_session_state()` method signature (updated in Sprint 2.5)
2. Fix test to use correct calling convention: `get_session_state(session_id)` or `get_session_state(db, session_id)`
3. Ensure Redis client is properly awaited in async context
4. Validate session_manager is used correctly

**Acceptance Criteria:**
- [ ] `test_session_state_retrieval_performance` passes
- [ ] Redis client properly handles async/sync contexts
- [ ] Performance benchmarks meet <500ms target

**Validation Command:**
```bash
cd backend
pytest tests/services/agent/integration/test_performance.py::test_session_state_retrieval_performance -v --tb=long
```

**Expected Outcome:** 1/1 passing (currently 0/1)

---

#### **PRIORITY 2: Investigate Security Test Errors**
**File:** [backend/tests/services/agent/integration/test_security.py](backend/tests/services/agent/integration/test_security.py) (presumed)  
**Current Status:** 4 errors in agent test domain  
**Issue:** Unknown - requires investigation

**Tasks:**
1. Identify which tests are generating the 4 errors
2. Capture full error stacktraces
3. Fix or document if tests are aspirational (not yet implemented)
4. Add security validation patterns to agent tools

**Acceptance Criteria:**
- [ ] 4 security-related errors resolved or documented
- [ ] Agent input validation tested
- [ ] Rate limiting patterns validated

**Validation Command:**
```bash
cd backend
pytest tests/services/agent/ -k security -v --tb=long
```

**Expected Outcome:** 0 errors, tests pass or are explicitly marked as xfail

---

#### **PRIORITY 3: Agent-Data Integration**
**Files:**
- [backend/app/services/agent/tools/data_retrieval.py](backend/app/services/agent/tools/data_retrieval.py) (create or enhance)
- [backend/app/services/agent/agents/retrieval.py](backend/app/services/agent/agents/retrieval.py)

**Current Status:** Agent tools exist but may not query all 4 ledgers  
**Goal:** Ensure agents can query Glass, Human, Catalyst, Exchange ledgers

**Tasks:**
1. Review existing data retrieval tools
2. Create tools to query each ledger:
   - `get_price_data(symbol, timeframe)` â†’ Glass ledger
   - `get_sentiment_data(symbol, source)` â†’ Human ledger
   - `get_catalyst_events(symbol, event_type)` â†’ Catalyst ledger
   - `get_order_history(user_id)` â†’ Exchange ledger
3. Implement SQLModel queries using unidirectional relationship pattern (see Sprint 2.5 learnings)
4. Create integration tests: `tests/services/agent/integration/test_data_integration.py`

**Acceptance Criteria:**
- [ ] 4 data retrieval tools implemented (one per ledger)
- [ ] Tools handle missing data gracefully
- [ ] 10+ integration tests validate agent can query all ledgers
- [ ] No SQLModel relationship warnings

**Validation Command:**
```bash
cd backend
pytest tests/services/agent/integration/test_data_integration.py -v
```

**Expected Outcome:** 10+ new passing tests for data integration

---

#### **PRIORITY 4: Document Agent Query Capabilities**
**File:** [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) - Add Section 10: Agent-Data Interface

**Tasks:**
1. Document available agent tools and their parameters
2. Provide example queries for each ledger
3. Document data access patterns and performance considerations
4. Link to test files showing usage examples

**Acceptance Criteria:**
- [ ] Section 10 added to ARCHITECTURE.md
- [ ] All 4 ledgers documented with example queries
- [ ] Performance guidelines documented (<1s per query target)

**Validation:**
- Review by Track A developer for accuracy
- Validate examples work in actual agent workflow

---

### ðŸ“Š Track B Test Targets

**Baseline (Current):**
- Total: 565 passing, 18 failing, 77 errors
- Your domain: ~250 tests (agent unit + integration)
- Current pass rate: 19/20 integration tests

**Sprint 2.6 Target:**
- Fix 5 issues (1 Redis performance + 4 security errors)
- Add 10+ data integration tests
- **End State:** ~260 passing tests in your domain, 20/20 integration tests

**Daily Validation:**
```bash
cd backend
# Quick check your test domains
pytest tests/services/agent/integration/ --tb=short -q

# Full baseline comparison
pytest tests/services/agent/ --tb=short --quiet | tee test_output.txt
```

---

### ðŸ”— Integration Points

**Track A â†’ Your Inputs:**
- Seed data fixes (Track A P1) will provide valid test data for your agent queries
- Quality monitor (Track A P3) results may be queryable by agents
- Schema stability maintained by Track A

**Your Outputs â†’ Track A:**
- Document any new model fields needed for agent queries
- Coordinate on read-only access patterns for ledger tables

**Track C â†’ Your Dependencies:**
- **CRITICAL:** Secrets management (Track C P1) needed for OPENAI_API_KEY
- Until secrets deployed, use mock LLM responses in tests

---

### ðŸ“ Definition of Done

**Code Quality:**
- [ ] All modified code passes `bash scripts/format.sh`
- [ ] All modified code passes `bash scripts/lint.sh`
- [ ] No event loop nesting issues in async agent code

**Testing:**
- [ ] Redis performance test fixed (1 test)
- [ ] Security errors resolved (4 errors)
- [ ] Data integration tests implemented (10+ tests)
- [ ] 20/20 integration tests passing

**Documentation:**
- [ ] Section 10 added to ARCHITECTURE.md (Agent-Data Interface)
- [ ] Tool docstrings complete with parameter descriptions
- [ ] Update CURRENT_SPRINT.md with final test counts

**Commit Strategy:**
- Commit P1 (Redis fix) and P2 (security) separately
- Commit P3 (data integration) as single feature
- Create PR titled "Track B Sprint 2.6: Test Fixes + Agent-Data Integration"

---

## ðŸ—ï¸ Developer C: DevOps Engineer Sprint 2.6

**Role:** OMC-DevOps-Engineer  
**Focus:** Terraform Secrets Module + Production Readiness

---

### ðŸ“š Context Review (Read These First)

1. **Sprint History:** [CURRENT_SPRINT.md](CURRENT_SPRINT.md) lines 80-216 - Review Track C accomplishments and P2 items
2. **Infrastructure Overview:** [infrastructure/terraform/README.md](infrastructure/terraform/README.md) - ECS architecture (not EKS)
3. **Deployment Guide:** [infrastructure/terraform/DEPLOYMENT_GUIDE_TERRAFORM_ECS.md](infrastructure/terraform/DEPLOYMENT_GUIDE_TERRAFORM_ECS.md)

---

### ðŸŽ¯ Sprint 2.6 Objectives

#### **PRIORITY 1: Implement Terraform Secrets Module** (P2 from Sprint 2.5)
**Directory:** [infrastructure/terraform/modules/secrets/](infrastructure/terraform/modules/secrets/) (currently EMPTY)  
**Current Status:** Directory exists but contains no files  
**Issue:** AWS Secrets Manager integration incomplete, blocking OPENAI_API_KEY injection

**Tasks:**
1. Create `modules/secrets/main.tf`:
   - Define `aws_secretsmanager_secret` resources for:
     - `omc/openai/api_key`
     - `omc/database/credentials`
     - `omc/redis/connection_string`
   - Define `aws_secretsmanager_secret_version` for initial values
   - Create IAM policy for ECS task role secret access

2. Create `modules/secrets/variables.tf`:
   - `secret_name` (string)
   - `secret_value` (string, sensitive)
   - `environment` (string: staging/production)
   - `recovery_window_in_days` (number, default 30)

3. Create `modules/secrets/outputs.tf`:
   - `secret_arn` (output)
   - `secret_name` (output)
   - `iam_policy_arn` (output for ECS task role attachment)

4. Update `environments/staging/main.tf` to use secrets module:
   ```hcl
   module "secrets" {
     source      = "../../modules/secrets"
     environment = "staging"
     secret_name = "omc/openai/api_key"
     secret_value = var.openai_api_key  # from variables
   }
   ```

5. Update ECS task definitions to inject secrets as environment variables

**Acceptance Criteria:**
- [ ] Secrets module contains main.tf, variables.tf, outputs.tf
- [ ] Module creates secrets with proper KMS encryption
- [ ] ECS task role has IAM policy attached for secret access
- [ ] Secrets injected into ECS containers as environment variables
- [ ] Manual validation: `terraform plan` in staging shows secret resources

**Validation Command:**
```bash
cd infrastructure/terraform/environments/staging
terraform init
terraform plan -out=tfplan
terraform show tfplan | grep secretsmanager
```

**Expected Outcome:** Secret resources shown in plan, no errors

---

#### **PRIORITY 2: Deployment Automation**
**Files:**
- `.github/workflows/deploy.yml` (create or enhance)
- `scripts/deploy.sh` (exists, may need updates)

**Current Status:** CI/CD pipeline exists but may not fully automate ECS deployment  
**Goal:** One-command deployment to staging/production

**Tasks:**
1. Review existing `scripts/deploy.sh` and `.github/workflows/` files
2. Enhance GitHub Actions workflow:
   - Build Docker images (backend, frontend)
   - Push to ECR
   - Update ECS service with new task definition
   - Wait for deployment stability
3. Add deployment gates:
   - Require passing tests before deployment
   - Manual approval for production
4. Create rollback procedure documentation

**Acceptance Criteria:**
- [ ] `scripts/deploy.sh` deploys to staging with single command
- [ ] GitHub Actions workflow deploys on merge to main
- [ ] Deployment waits for ECS service to stabilize
- [ ] Rollback procedure documented in OPERATIONS_RUNBOOK.md

**Validation Command:**
```bash
# Test staging deployment
bash scripts/deploy.sh staging

# Check ECS service status
aws ecs describe-services --cluster omc-staging --services omc-backend
```

**Expected Outcome:** Deployment completes successfully, ECS tasks running

---

#### **PRIORITY 3: Production Deployment Runbook**
**File:** [infrastructure/terraform/OPERATIONS_RUNBOOK.md](infrastructure/terraform/OPERATIONS_RUNBOOK.md) (exists, enhance)

**Tasks:**
1. Review and update existing runbook
2. Add sections:
   - **Pre-Deployment Checklist** (secrets configured, tests passing, backups taken)
   - **Deployment Steps** (step-by-step with exact commands)
   - **Health Check Validation** (how to verify deployment success)
   - **Rollback Procedure** (how to revert to previous version)
   - **Troubleshooting** (common issues and solutions)
3. Validate runbook by performing staging deployment following documented steps

**Acceptance Criteria:**
- [ ] Runbook contains all 5 sections
- [ ] Commands tested and validated on staging
- [ ] Rollback procedure successfully executed (test)
- [ ] Health check endpoints documented with expected responses

**Validation:**
- Perform end-to-end staging deployment using only runbook instructions
- Document any gaps or errors found

---

#### **PRIORITY 4: CloudWatch Monitoring Setup**
**Files:**
- `infrastructure/terraform/modules/monitoring/` (create if doesn't exist)
- Update `environments/staging/main.tf` to use monitoring module

**Tasks:**
1. Create CloudWatch dashboards for:
   - ECS Container Insights (CPU, Memory, Network)
   - RDS Performance (Connections, Query Duration)
   - Redis Metrics (Hit Rate, Evictions)
   - Application Logs (Error Rate, Latency)
2. Create CloudWatch alarms for:
   - High CPU (>80% for 5 minutes)
   - High Memory (>80% for 5 minutes)
   - Failed Health Checks (>3 in 5 minutes)
   - 5xx Error Rate (>5% in 5 minutes)
3. Configure SNS topic for alarm notifications

**Acceptance Criteria:**
- [ ] CloudWatch dashboard created with 4+ widgets
- [ ] 4+ alarms configured with appropriate thresholds
- [ ] SNS topic configured (can use dummy email for now)
- [ ] Dashboard accessible via AWS console

**Validation:**
- Open CloudWatch dashboard and verify metrics are populated
- Trigger test alarm by simulating high CPU (optional)

---

### ðŸ“Š Track C Deliverables

**Baseline (Current):**
- ECS base deployment: âœ… Complete
- Secrets module: âš ï¸ Empty directory
- Deployment automation: ðŸŸ¡ Partial
- Monitoring: ðŸŸ¡ Basic health checks only

**Sprint 2.6 Target:**
- Secrets module: âœ… Complete with 3 secrets (OpenAI, DB, Redis)
- Deployment automation: âœ… One-command staging deployment
- Runbook: âœ… Validated on staging
- Monitoring: âœ… Dashboards and alarms configured

**Daily Validation:**
```bash
# Check Terraform modules structure
ls -la infrastructure/terraform/modules/secrets/

# Validate Terraform configuration
cd infrastructure/terraform/environments/staging
terraform validate

# Check ECS service status
aws ecs describe-services --cluster omc-staging --services omc-backend --query 'services[0].deployments'
```

---

### ðŸ”— Integration Points

**Your Outputs â†’ Track A:**
- Secrets management enables DB credential injection (eventually)
- CloudWatch logs available for debugging collector issues

**Your Outputs â†’ Track B:**
- **CRITICAL:** OPENAI_API_KEY injection unblocks agent LLM testing
- Monitoring dashboards show agent performance metrics

**Track A/B â†’ Your Inputs:**
- No blockers: infrastructure work is independent
- Coordinate deployment timing after Track A/B test fixes merged

---

### ðŸ“ Definition of Done

**Infrastructure:**
- [ ] Secrets module complete (3 files created)
- [ ] Secrets injected into ECS tasks successfully
- [ ] Deployment automation tested on staging
- [ ] CloudWatch dashboard operational

**Documentation:**
- [ ] OPERATIONS_RUNBOOK.md updated with all 5 sections
- [ ] Runbook validated by performing staging deployment
- [ ] Monitoring dashboard screenshots added to runbook

**Security:**
- [ ] Secrets encrypted with KMS
- [ ] IAM policies follow least-privilege principle
- [ ] Secret rotation policy documented (even if not automated)

**Commit Strategy:**
- Commit P1 (secrets module) as single feature
- Commit P2/P3 (deployment automation + runbook) together
- Commit P4 (monitoring) separately
- Create PR titled "Track C Sprint 2.6: Secrets Module + Deployment Automation"

---

## ðŸ”„ Sprint Execution Guidelines

### Daily Standup (Async via Comments)
Each developer posts daily update to GitHub Discussion or Sprint board:
```
**Yesterday:** [What you completed]
**Today:** [What you're working on]
**Blockers:** [Any issues blocking progress]
**Test Status:** [Current pass/fail/error counts in your domain]
```

### Mid-Sprint Check-In (Day 7)
All developers post:
- Test progress vs. targets
- Any discovered scope changes
- Risks to sprint completion

### End-of-Sprint Review (Day 14)
All developers:
1. Merge PRs to main (must pass CI/CD)
2. Run full test suite on merged main
3. Update CURRENT_SPRINT.md with final status
4. Document any P2 items for Sprint 2.7

---

## ðŸ“ Testing Protocol

### Pre-Commit Testing
```bash
cd backend
bash scripts/format.sh    # Auto-format code
bash scripts/lint.sh      # Check linting
pytest tests/[your-domain]/ --tb=short -q  # Run your tests
```

### Daily Baseline Comparison
```bash
cd backend
pytest --tb=short --quiet > test_output_$(date +%Y%m%d).txt
grep -E "passed|failed|error" test_output_$(date +%Y%m%d).txt
```

### End-of-Sprint Validation
```bash
# Clean environment
docker compose down -v
docker compose up -d db redis
docker compose run --rm prestart

# Full test suite
cd backend
pytest --tb=short --quiet | tee sprint_2.6_final_tests.txt

# Compare to baseline
echo "Baseline: 565 passing, 18 failing, 77 errors"
grep -E "passed|failed|error" sprint_2.6_final_tests.txt
```

---

## ðŸŽ“ Reference Materials

### Technical Constraints (Learned in Sprint 2.5)
- **SQLModel Relationships:** Use unidirectional relationships only (see [ARCHITECTURE.md Section 9](docs/ARCHITECTURE.md#section-9-technical-constraints-best-practices))
- **Async Mocks:** Use `MagicMock` for callables returning context managers (see [TESTING.md](docs/TESTING.md))
- **Schema Validation:** ARRAY fields require `postgresql.ARRAY(sa.String())` not `list[str]`

### Testing Patterns
- **Integration Tests:** Require running services (db, redis)
- **Isolation:** Use `pytest -k <pattern>` to run specific test groups
- **Debugging:** Use `--tb=long` for full stacktraces, `--pdb` for interactive debugging

### Git Workflow
```bash
# Start work on your track
git checkout main
git pull origin main
git checkout -b track-[a|b|c]-sprint-2.6

# Daily commits
git add .
git commit -m "feat: descriptive message"
git push origin track-[a|b|c]-sprint-2.6

# End of sprint
# Create PR: "Track [A|B|C] Sprint 2.6: [Brief Summary]"
# Wait for CI/CD to pass
# Request review from other developers
# Merge when approved
```

---

## ðŸ“ž Support & Escalation

**Blockers:**
- If blocked >24 hours, escalate via GitHub Discussion
- Tag relevant developer if cross-track coordination needed

**Scope Changes:**
- Propose scope changes by Day 3 (before mid-sprint)
- Document rationale and impact on test targets

**Questions:**
- Check [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) and [docs/TESTING.md](docs/TESTING.md) first
- Search previous sprint documentation in `docs/archive/history/`
- Ask via GitHub Discussion if answers not found

---

## âœ… Sprint Completion Checklist

**All Developers:**
- [ ] PR created and passing CI/CD
- [ ] Test targets met or deviations documented
- [ ] Code formatted and linted
- [ ] Documentation updated (ARCHITECTURE.md if needed)

**Track A:**
- [ ] 7 seed data tests fixed
- [ ] 20 PnL errors resolved
- [ ] Quality monitor implemented with 10+ tests

**Track B:**
- [ ] 1 Redis performance test fixed
- [ ] 4 security errors resolved
- [ ] 10+ data integration tests added
- [ ] Agent-Data Interface documented

**Track C:**
- [ ] Secrets module complete (3 files)
- [ ] Deployment automation tested
- [ ] Operations runbook validated
- [ ] CloudWatch monitoring configured

**Sprint Close:**
- [ ] All PRs merged to main
- [ ] Full test suite run on merged main
- [ ] CURRENT_SPRINT.md updated with final status
- [ ] Sprint retrospective completed

---

**Ready to begin Sprint 2.6? Each developer should:**
1. Read this entire document
2. Review the 5 context files listed in "Context Analysis Required"
3. Set up local test environment (see [TESTING.md](docs/TESTING.md))
4. Create your track branch
5. Post "Sprint 2.6 Started" update with Day 1 plan

Good luck! ðŸš€
